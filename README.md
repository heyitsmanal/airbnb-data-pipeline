# ğŸ  Airbnb Data Pipeline (Python â€¢ Airflow â€¢ Docker â€¢ PostgreSQL)

A **production-style ETL pipeline** designed to automate the ingestion, transformation, and loading of Airbnb listings data into a PostgreSQL data warehouse.  
The workflow is fully **containerized with Docker** and orchestrated using **Apache Airflow** for daily scheduling and monitoring.

---

## âš™ï¸ Project Overview

The **Airbnb Data Pipeline** automates the following key processes:

1. **Extract** â€“ Ingests Airbnb listings from CSV files or a public API  
2. **Transform** â€“ Cleans, enriches, and validates the data using **pandas**  
3. **Load** â€“ Loads the processed data into a **PostgreSQL** warehouse using **SQLAlchemy**

This setup demonstrates how to design a modular, scalable, and reproducible ETL process suitable for real-world data engineering environments.

---

## ğŸš€ Technologies Used

- **Python** â€“ Data manipulation, validation, and scripting  
- **Apache Airflow** â€“ Workflow orchestration and task scheduling  
- **PostgreSQL** â€“ Centralized data storage and modeling  
- **SQLAlchemy** â€“ ORM for efficient database interaction  
- **Docker & Docker Compose** â€“ Containerization for portability and consistency  

---

## ğŸ“Š Airflow DAG in Action

Below is a snapshot of the pipeline running in **Apache Airflow**, showing the three core ETL tasks executed in sequence:

![Airbnb DAG Screenshot](https://github.com/user-attachments/assets/fd92e0c3-7167-4d81-a3da-d15d639de2cf)

Each DAG run performs:
- **Extract:** Fetch raw Airbnb data  
- **Transform:** Apply cleaning and enrichment logic  
- **Load:** Store refined data in PostgreSQL  

---

## ğŸ¯ Purpose & Key Takeaways

This project highlights hands-on experience with modern **Data Engineering** practices, including:
- Workflow automation using **Airflow DAGs**  
- Data extraction and cleaning with **pandas**  
- Data modeling and loading into **PostgreSQL**  
- Deployment via **Docker Compose** for easy reproducibility  

It serves as a demonstration of building a **scalable, maintainable ETL pipeline** from scratch â€” a valuable foundation for production-grade data workflows.

---

## ğŸ§  Future Improvements

- Add data quality validation with **Great Expectations**  
- Integrate automated logging and alerting  
- Extend to real-time ingestion using **Kafka** or **Spark Streaming**

---
